# AI-Powered Local Knowledge Base & RAG System

This project is a proof-of-concept demonstrating an end-to-end pipeline for building and querying a local knowledge base using Retrieval Augmented Generation (RAG) with Large Language Models (LLMs). It involves scripts for data scraping/ingestion from various sources, text processing and chunking, embedding generation, storage in a vector database, and a RAG-based query interface.

## Project Overview

The system performs the following key functions:
1.  **Data Acquisition:** Scripts to download or process data from YouTube, the CISA KEV catalog, local PDF documents, and Internet Archive collections.
2.  **Data Processing & Chunking:** Scripts to parse the acquired data, extract relevant text, and split it into manageable chunks suitable for embedding.
3.  **Vector Database Ingestion:** A script to generate embeddings from the text chunks and store them in a local ChromaDB instance.
4.  **RAG Query System:** A script that allows users to ask natural language questions, retrieves relevant information from the vector database, and uses an LLM (via Ollama) to generate answers based on the retrieved context.

## Scripts

This repository contains the following Python scripts:

### Data Acquisition & Initial Processing:

* **`scrape_youtube_metadata.py`**:
    * Scrapes metadata and transcripts for videos from specified YouTube channels.
    * Uses `yt-dlp` for fetching video lists and metadata, and `youtube-transcript-api` for retrieving transcripts.
    * Saves the extracted information (including transcript text, video title, URL, upload date, etc.) into JSON files, organized by channel.
    * Maintains a log of processed videos to allow for incremental scraping.

* **`download_ia_collection.py`**:
    * Downloads files (e.g., PDFs by default) from specified collections on the Internet Archive.
    * Utilizes the `internetarchive` command-line tool (`ia`) to perform the download operations.
    * Supports specifying collection identifiers, file patterns to download, and exclusion patterns.

### Data Chunking (Preparing for Ingestion):

These scripts take raw data from the acquisition phase and process it into a standardized JSONL format (`knowledge_base_chunks.jsonl`), where each line represents a text chunk with its metadata. They use LangChain's `RecursiveCharacterTextSplitter` for chunking.

* **`chunk_cisa_kev.py`**:
    * Processes the CISA Known Exploited Vulnerabilities (KEV) catalog from its JSON data file.
    * Extracts relevant fields for each vulnerability (e.g., CVE ID, vulnerability name, description, required action).
    * Chunks the combined textual information for each CVE entry.
    * Logs processed CVE IDs to support incremental updates.

* **`chunk_local_pdfs.py`**:
    * Extracts text content from PDF files located in a specified input directory (and its subdirectories).
    * Uses the `PyMuPDF (fitz)` library for robust PDF parsing.
    * Chunks the extracted text from each PDF.
    * Maintains a log of processed PDF file paths for incremental processing.

* **`chunk_youtube_transcripts.py`**:
    * Processes the JSON files generated by `scrape_youtube_metadata.py` (which contain the video transcripts).
    * Extracts the transcript text and relevant metadata (video ID, title, channel, etc.).
    * Chunks the transcript text for each video.
    * Logs processed transcript files to enable incremental updates.

### Vector Database Ingestion:

* **`ingest_chunks_to_chromadb.py`**:
    * Reads the processed and chunked data from the shared JSONL file (`knowledge_base_chunks.jsonl`).
    * Generates text embeddings for each chunk using a HuggingFace sentence transformer model (e.g., `BAAI/bge-base-en-v1.5`).
    * Ingests these embeddings along with their corresponding text and metadata into a local ChromaDB persistent vector database.
    * Includes logic to check for existing document IDs in the database to avoid duplicates during incremental ingestion.

### Querying & RAG System:

* **`query_rag_database.py`**:
    * Provides a basic command-line interface (CLI) for interacting with the knowledge base.
    * Takes a user's query, retrieves relevant document chunks from the ChromaDB vector store using semantic search.
    * Uses an Ollama-hosted Large Language Model (LLM) (e.g., Llama3, Mistral) to synthesize an answer based on the retrieved context and chat history (Retrieval Augmented Generation).
    * Includes features like chat memory and optional re-ranking of retrieved documents using a CrossEncoder model.

* **`Auto_query_rag_database.py`**:
    * An advanced version of the RAG query script.
    * Features more sophisticated control over the retrieval and re-ranking process, including interactive thresholding for reranked results to allow dynamic adjustment of relevance filtering per query.
    * Manages chat sessions, different operational modes (RAG vs. retrieval-only), and logs conversations.

## Note

These scripts are designed to be run in an environment where the necessary Python packages (e.g., LangChain, HuggingFace Transformers, ChromaDB, Ollama, yt-dlp, PyMuPDF, etc.) are installed. External tools like `ffmpeg` (for `yt-dlp`) and the `ia` CLI (for `download_ia_collection.py`) may also be required depending on the scripts used. The scripts often expect a specific project directory structure for input data, output files, and logs, as indicated within the scripts themselves.
